{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5hqxwtZ8IcG"
      },
      "source": [
        "## Chat Models - Router Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NowmDFg_8IcH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "%pip install langchain\n",
        "os.environ['OPENAI_API_KEY'] = 'API_KEY_HERE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op55pTRg8IcH"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rggAeIzp8IcH"
      },
      "outputs": [],
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
        "You are so good because you are able to break down hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPfCIDux8IcH"
      },
      "outputs": [],
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template,\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oknl-wi8IcH"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5DviVX98IcH"
      },
      "outputs": [],
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "default_chain = ConversationChain(llm=llm, output_key=\"text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep4i2UZa8IcH",
        "outputId": "358fe386-a601-4fe7-8939-31eae45e4ac2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'physics': LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template=\"You are a very smart physics professor. You are great at answering questions about physics in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.\\n\\nHere is a question:\\n{input}\", template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.7, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}),\n",
              " 'math': LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, answer the component parts, and then put them together to answer the broader question.\\n\\nHere is a question:\\n{input}', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.7, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={})}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "destination_chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HghWEb88IcI"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj094jDf8IcI"
      },
      "outputs": [],
      "source": [
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLCWtDzf8IcI",
        "outputId": "0ef6f665-37cf-49a8-addf-fc1b8c81d90f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['input'], output_parser=RouterOutputParser(default_destination='DEFAULT', next_inputs_type=<class 'str'>, next_inputs_inner_key='input'), partial_variables={}, template='Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{\\n    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\ a potentially modified version of the original input\\n}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nphysics: Good for answering questions about physics\\nmath: Good for answering math questions\\n\\n<< INPUT >>\\n{input}\\n\\n<< OUTPUT (must include ```json at the start of the response) >>\\n', template_format='f-string', validate_template=True)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "router_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvVYxVN18IcI"
      },
      "outputs": [],
      "source": [
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain,\n",
        "    destination_chains=destination_chains,\n",
        "    default_chain=default_chain,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBsZ9ymt8IcI",
        "outputId": "c5a65aba-b9c4-48a9-c7af-5e51ef018311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an idealized object that absorbs all radiation incident upon it and emits radiation at all wavelengths. It is a fundamental concept in physics and plays a crucial role in understanding various phenomena, including the behavior of stars, the thermal radiation from objects, and the formation of the cosmic microwave background radiation.\n",
            "\n",
            "Black body radiation is characterized by its spectral distribution, known as Planck's law, which describes the intensity of radiation at different wavelengths and temperatures. The shape of this distribution depends solely on the temperature of the object, with hotter objects emitting more radiation at all wavelengths. At room temperature, the peak of the distribution lies in the infrared range, while at higher temperatures it shifts towards visible light and even into the ultraviolet range.\n",
            "\n",
            "One key aspect of black body radiation is that it exhibits a continuous spectrum, meaning it contains all possible wavelengths of electromagnetic radiation. This is different from the emission spectra of specific elements or molecules, which consist of discrete lines at certain wavelengths. The continuous nature of black body radiation was one of the puzzles that led to the development of quantum mechanics.\n",
            "\n",
            "Overall, black body radiation is a fundamental concept in physics that helps us understand the emission and absorption of electromagnetic radiation by objects, providing insights into a wide range of phenomena in the universe.\n"
          ]
        }
      ],
      "source": [
        "print(chain.run(\"What is black body radiation?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXSVYdEH8IcI",
        "outputId": "19f6ba75-b877-4bd4-fa69-b7d48ea96a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "math: {'input': 'What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Thank you for the compliment! I'll be happy to help you with the math question.\n",
            "\n",
            "To find the first prime number greater than 40, we will start by listing the prime numbers greater than 40 and checking if one plus the prime number is divisible by 3.\n",
            "\n",
            "Prime numbers greater than 40: 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, ...\n",
            "\n",
            "Let's check if one plus each of these prime numbers is divisible by 3:\n",
            "\n",
            "41 + 1 = 42 (divisible by 3)\n",
            "43 + 1 = 44 (not divisible by 3)\n",
            "47 + 1 = 48 (divisible by 3)\n",
            "53 + 1 = 54 (divisible by 3)\n",
            "59 + 1 = 60 (divisible by 3)\n",
            "61 + 1 = 62 (not divisible by 3)\n",
            "67 + 1 = 68 (not divisible by 3)\n",
            "71 + 1 = 72 (divisible by 3)\n",
            "73 + 1 = 74 (not divisible by 3)\n",
            "79 + 1 = 80 (not divisible by 3)\n",
            "83 + 1 = 84 (divisible by 3)\n",
            "89 + 1 = 90 (divisible by 3)\n",
            "97 + 1 = 98 (not divisible by 3)\n",
            "\n",
            "From this list, we can see that the first prime number greater than 40 such that one plus the prime number is divisible by 3 is 41. Therefore, 41 is the answer to the given question.\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    chain.run(\n",
        "        \"What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\"\n",
        "    )\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}